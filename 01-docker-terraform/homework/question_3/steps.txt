Step 1: Create docker-compose.yaml

Step 2: Start the containers by running docker compose up (-d)

Step 3: Navigate to pgadmin web ui via browser (localhost:8080)

Step 4: Create a new db connection with credentials:
Host name/address: pgdatabase
Port: 5432
Username: root
Password: root

Step 5: Prepare scripts (ingest_zone_lookup.py, ingest_green_trip_data.py)

Step 6: Create docker image for ingesting the zone lookup data by creating an image with the following Dockerfile:

------------------------------------------------
FROM python:3.9

RUN apt-get install wget
RUN pip install pandas sqlalchemy psycopg2

WORKDIR /app

COPY ingest_zone_lookup.py ./

ENTRYPOINT ["python", "ingest_zone_lookup.py"]
------------------------------------------------

Now build the image:
docker build -t ingest_zone_lookup:v001 .

Finally, run the containerized script:
docker run -it --network pg_network ingest_zone_lookup:v001 --username=root --password=root --host=pgdatabase --port=5432 --db=ny_taxi --table=taxi_zone_lookup --url="https://github.com/DataTalksClub/nyc-tlc-data/releases/download/misc/taxi_zone_lookup.csv"

Step 7: Create docker image for ingesting the green taxi trip data for October 2019 by creating an image with the following Dockerfile:

------------------------------------------------
FROM python:3.9

RUN apt-get install wget
RUN pip install pandas sqlalchemy psycopg2

WORKDIR /app

COPY ingest_green_trip_data.py ./

ENTRYPOINT ["python", "ingest_green_trip_data.py"]
------------------------------------------------

Now build the image:
docker build -t ingest_green_trips:v001 .

Finally, run the containerized script:
sudo docker run -it --network pg_network ingest_green_trips:v001 --username=root --password=root --host=pgdatabase --port=5432 --db=ny_taxi --table=green_taxi_data_2019_10 --url="https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-10.csv.gz"

Step 8: See question_3.sql